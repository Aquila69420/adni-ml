{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T09:40:17.041305Z",
     "start_time": "2025-07-28T09:40:12.708467Z"
    }
   },
   "source": [
    "import transformers\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import os\n",
    "from nilearn.image import load_img\n",
    "import pandas as pd\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "data_dir = 'data'\n",
    "pet_dir = \"data/ad_pet_huw\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:26.235099Z",
     "start_time": "2025-07-24T11:27:23.205742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = {}\n",
    "for file in os.listdir(pet_dir):\n",
    "    if file.endswith(\".nii\"):\n",
    "        img = load_img(os.path.join(pet_dir, file))\n",
    "        patient_id = file.split(\".\")[0].removeprefix('AD_normalised_')\n",
    "        print(f'Processing patient: {patient_id}')\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        torch_img = torch.tensor(img.get_fdata(), dtype=torch.float32)\n",
    "        files[patient_id] =torch_img"
   ],
   "id": "747bf61390b1b9ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient: 002_S_5018\n",
      "Processing patient: 003_S_4136\n",
      "Processing patient: 003_S_4152\n",
      "Processing patient: 003_S_4373\n",
      "Processing patient: 003_S_4892\n",
      "Processing patient: 003_S_5165\n",
      "Processing patient: 003_S_5187\n",
      "Processing patient: 005_S_4707\n",
      "Processing patient: 005_S_4910\n",
      "Processing patient: 005_S_5038\n",
      "Processing patient: 005_S_5119\n",
      "Processing patient: 006_S_4153\n",
      "Processing patient: 006_S_4192\n",
      "Processing patient: 006_S_4546\n",
      "Processing patient: 006_S_4867\n",
      "Processing patient: 007_S_4568\n",
      "Processing patient: 007_S_4637\n",
      "Processing patient: 007_S_4911\n",
      "Processing patient: 007_S_5196\n",
      "Processing patient: 009_S_5027\n",
      "Processing patient: 009_S_5037\n",
      "Processing patient: 009_S_5224\n",
      "Processing patient: 009_S_5252\n",
      "Processing patient: 011_S_4827\n",
      "Processing patient: 011_S_4845\n",
      "Processing patient: 011_S_4906\n",
      "Processing patient: 011_S_4912\n",
      "Processing patient: 011_S_4949\n",
      "Processing patient: 013_S_5071\n",
      "Processing patient: 014_S_4039\n",
      "Processing patient: 014_S_4615\n",
      "Processing patient: 016_S_4009\n",
      "Processing patient: 016_S_4353\n",
      "Processing patient: 016_S_4583\n",
      "Processing patient: 016_S_4591\n",
      "Processing patient: 016_S_4887\n",
      "Processing patient: 016_S_4963\n",
      "Processing patient: 016_S_5032\n",
      "Processing patient: 016_S_5057\n",
      "Processing patient: 016_S_5251\n",
      "Processing patient: 018_S_4696\n",
      "Processing patient: 018_S_5240\n",
      "Processing patient: 019_S_4252\n",
      "Processing patient: 019_S_4477\n",
      "Processing patient: 019_S_4549\n",
      "Processing patient: 019_S_5012\n",
      "Processing patient: 019_S_5019\n",
      "Processing patient: 021_S_4718\n",
      "Processing patient: 021_S_4924\n",
      "Processing patient: 023_S_4501\n",
      "Processing patient: 023_S_5120\n",
      "Processing patient: 023_S_5241\n",
      "Processing patient: 024_S_4223\n",
      "Processing patient: 024_S_4280\n",
      "Processing patient: 024_S_4905\n",
      "Processing patient: 024_S_5054\n",
      "Processing patient: 027_S_4801\n",
      "Processing patient: 027_S_4802\n",
      "Processing patient: 027_S_4938\n",
      "Processing patient: 027_S_4962\n",
      "Processing patient: 027_S_4964\n",
      "Processing patient: 029_S_4307\n",
      "Processing patient: 031_S_4024\n",
      "Processing patient: 032_S_4755\n",
      "Processing patient: 033_S_5013\n",
      "Processing patient: 033_S_5017\n",
      "Processing patient: 033_S_5087\n",
      "Processing patient: 035_S_4783\n",
      "Processing patient: 036_S_4740\n",
      "Processing patient: 036_S_4820\n",
      "Processing patient: 036_S_4894\n",
      "Processing patient: 036_S_5063\n",
      "Processing patient: 036_S_5112\n",
      "Processing patient: 036_S_5210\n",
      "Processing patient: 037_S_4001\n",
      "Processing patient: 037_S_4770\n",
      "Processing patient: 037_S_4879\n",
      "Processing patient: 037_S_5162\n",
      "Processing patient: 051_S_4980\n",
      "Processing patient: 051_S_5005\n",
      "Processing patient: 052_S_4959\n",
      "Processing patient: 052_S_5062\n",
      "Processing patient: 053_S_5070\n",
      "Processing patient: 053_S_5208\n",
      "Processing patient: 067_S_4728\n",
      "Processing patient: 067_S_5205\n",
      "Processing patient: 068_S_4859\n",
      "Processing patient: 068_S_4968\n",
      "Processing patient: 068_S_5146\n",
      "Processing patient: 070_S_4692\n",
      "Processing patient: 070_S_4719\n",
      "Processing patient: 073_S_4853\n",
      "Processing patient: 073_S_5016\n",
      "Processing patient: 073_S_5090\n",
      "Processing patient: 082_S_5029\n",
      "Processing patient: 082_S_5184\n",
      "Processing patient: 094_S_4089\n",
      "Processing patient: 094_S_4282\n",
      "Processing patient: 094_S_4737\n",
      "Processing patient: 098_S_4095\n",
      "Processing patient: 098_S_4201\n",
      "Processing patient: 098_S_4215\n",
      "Processing patient: 099_S_4994\n",
      "Processing patient: 100_S_5106\n",
      "Processing patient: 114_S_4379\n",
      "Processing patient: 116_S_4195\n",
      "Processing patient: 116_S_4209\n",
      "Processing patient: 116_S_4338\n",
      "Processing patient: 116_S_4625\n",
      "Processing patient: 116_S_4732\n",
      "Processing patient: 123_S_4526\n",
      "Processing patient: 126_S_4494\n",
      "Processing patient: 126_S_4686\n",
      "Processing patient: 127_S_4500\n",
      "Processing patient: 127_S_4940\n",
      "Processing patient: 127_S_4992\n",
      "Processing patient: 127_S_5028\n",
      "Processing patient: 127_S_5056\n",
      "Processing patient: 127_S_5058\n",
      "Processing patient: 127_S_5067\n",
      "Processing patient: 127_S_5095\n",
      "Processing patient: 128_S_4772\n",
      "Processing patient: 128_S_4774\n",
      "Processing patient: 128_S_4792\n",
      "Processing patient: 128_S_5123\n",
      "Processing patient: 130_S_4589\n",
      "Processing patient: 130_S_4641\n",
      "Processing patient: 130_S_4660\n",
      "Processing patient: 130_S_4730\n",
      "Processing patient: 130_S_4971\n",
      "Processing patient: 130_S_4982\n",
      "Processing patient: 130_S_4984\n",
      "Processing patient: 130_S_4990\n",
      "Processing patient: 130_S_4997\n",
      "Processing patient: 130_S_5006\n",
      "Processing patient: 130_S_5059\n",
      "Processing patient: 130_S_5231\n",
      "Processing patient: 131_S_5138\n",
      "Processing patient: 135_S_4657\n",
      "Processing patient: 135_S_4676\n",
      "Processing patient: 135_S_4863\n",
      "Processing patient: 135_S_4954\n",
      "Processing patient: 135_S_5015\n",
      "Processing patient: 135_S_5275\n",
      "Processing patient: 137_S_4211\n",
      "Processing patient: 137_S_4258\n",
      "Processing patient: 137_S_4672\n",
      "Processing patient: 137_S_4756\n",
      "Processing patient: 153_S_4172\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:26.837925Z",
     "start_time": "2025-07-24T11:27:26.648274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'ADNIMERGE_19Jun2025.csv'))\n",
    "sex_df = df.filter(['PTID', 'PTGENDER'])\n",
    "sex_map = {'Male': 0, 'Female': 1}\n",
    "sex_labels = {0: 'Male', 1: 'Female'}\n",
    "sex_df['PTGENDER'] = sex_df['PTGENDER'].map(sex_map)"
   ],
   "id": "79ae1aae983dd038",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv Khanna\\AppData\\Local\\Temp\\ipykernel_16032\\804732265.py:1: DtypeWarning: Columns (19,20,21,50,51,104,105,106) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(data_dir, 'ADNIMERGE_19Jun2025.csv'))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:29:18.845508Z",
     "start_time": "2025-07-24T11:29:18.819099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del df\n",
    "gc.collect()"
   ],
   "id": "2a374d69321325eb",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[43mdf\u001B[49m\n\u001B[32m      2\u001B[39m gc.collect()\n",
      "\u001B[31mNameError\u001B[39m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:30.126855Z",
     "start_time": "2025-07-24T11:27:30.103038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the number of common patients between the PET files and the sex_df\n",
    "missing_patients = sex_df[~sex_df['PTID'].isin(files.keys())]\n",
    "print(f'Missing patients: {len(missing_patients)}')\n",
    "\n",
    "common_patients = sex_df['PTID'].isin(files.keys())\n",
    "print(f'Common patients: {common_patients.sum()}')\n",
    "\n",
    "print(f'Total patients {len(files)}')"
   ],
   "id": "696db8e9b1a77e3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing patients: 15684\n",
      "Common patients: 737\n",
      "Total patients 149\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:31.299399Z",
     "start_time": "2025-07-24T11:27:31.114519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Update the sex DataFrame to include a new column for the PET image data matched on PTID\n",
    "for patient in files:\n",
    "    img = files.get(patient)\n",
    "    sex_df['PET_IMAGE'] = sex_df['PTID'].map(files) # Insert the img data ino the 'PET_IMAGE' column in sex_df for the corresponding PTID field"
   ],
   "id": "6972044d43a0ea82",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:31.995765Z",
     "start_time": "2025-07-24T11:27:31.987039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the PTID for the columns for which PET_IMAGE is not None\n",
    "print(f'Number of patients: {len(sex_df)}')\n",
    "sex_df.dropna(subset=['PET_IMAGE'], inplace=True)\n",
    "sex_df.drop_duplicates(subset=['PTID'], inplace=True)\n",
    "print(f'Number of patients with PET images: {len(sex_df)}')"
   ],
   "id": "f18888aef471ede9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 16421\n",
      "Number of patients with PET images: 149\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:28:27.690636Z",
     "start_time": "2025-07-24T11:28:27.684330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the shape of the PET image for the first patient\n",
    "first_patient = sex_df.iloc[0]\n",
    "print(f'First patient PTID: {first_patient[\"PTID\"]}')\n",
    "print(f'PET image shape: {first_patient[\"PET_IMAGE\"].shape} with data type {type(first_patient[\"PET_IMAGE\"])}')\n",
    "print(f'Sex: {first_patient[\"PTGENDER\"]}')"
   ],
   "id": "d9176747064842ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First patient PTID: 135_S_5275\n",
      "PET image shape: torch.Size([101, 116, 96]) with data type <class 'torch.Tensor'>\n",
      "Sex: 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:30:14.393862Z",
     "start_time": "2025-07-24T11:30:14.388023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select PET_IMAGE and PTGENDER from sex_df\n",
    "X = sex_df['PET_IMAGE'].tolist()  # This will be a list of torch.Tensor objects\n",
    "y = sex_df['PTGENDER'].values     # This will be a numpy array of labels\n",
    "\n",
    "print(f'X length: {len(X)}, PET image shape: {X[0].shape}, y shape: {y.shape}')"
   ],
   "id": "f04aba63a8c550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X length: 149, PET image shape: torch.Size([101, 116, 96]), y shape: (149,)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:30:23.594869Z",
     "start_time": "2025-07-24T11:30:23.587993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "print(f'Train X length: {len(X_train)}, Test X length: {len(X_test)} with shapes {X_train[0].shape}, {X_test[0].shape}')\n",
    "print(f'Train y shape: {y_train.shape}, Test y shape: {y_test.shape}')"
   ],
   "id": "26f6414232ff88f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X length: 119, Test X length: 30 with shapes torch.Size([101, 116, 96]), torch.Size([101, 116, 96])\n",
      "Train y shape: (119,), Test y shape: (30,)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T12:02:57.016074Z",
     "start_time": "2025-07-24T12:02:56.323797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs('data/pet_demographic', exist_ok=True)\n",
    "# Save the training and test data\n",
    "torch.save(X_train, 'data/pet_demographic/X_train.pt')\n",
    "torch.save(X_test, 'data/pet_demographic/X_test.pt')\n",
    "torch.save(y_train, 'data/pet_demographic/y_train.pt')\n",
    "torch.save(y_test, 'data/pet_demographic/y_test.pt')"
   ],
   "id": "d165e54d4a341253",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:40:20.237813Z",
     "start_time": "2025-07-28T09:40:19.948465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = torch.load('data/pet_demographic/X_train.pt', weights_only=False)\n",
    "X_test = torch.load('data/pet_demographic/X_test.pt', weights_only=False)\n",
    "y_train = torch.load('data/pet_demographic/y_train.pt', weights_only=False)\n",
    "y_test = torch.load('data/pet_demographic/y_test.pt', weights_only=False)"
   ],
   "id": "20306e9e0b9215df",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:40:21.198988Z",
     "start_time": "2025-07-28T09:40:21.186267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PETDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image.unsqueeze(0), label  # Add channel dimension for CNN input"
   ],
   "id": "fe93456d764bb871",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T12:49:04.642037Z",
     "start_time": "2025-07-24T12:49:04.631121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a 3D CNN to deal with images of shape (101, 116, 96)\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(1, 16, kernel_size=5, padding='valid')\n",
    "        self.conv2 = torch.nn.Conv3d(16, 32, kernel_size=5, padding='valid')\n",
    "        self.pool = torch.nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 46 * 54 * 44, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 1)  # Output layer for binary classification\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "id": "a4c1e2a517af1973",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:49:21.516362Z",
     "start_time": "2025-07-28T09:49:21.399630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "torch.cuda.empty_cache()\n",
    "print(f'Using device: {device}')"
   ],
   "id": "3350d0b3e8accc97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T17:49:56.057632Z",
     "start_time": "2025-07-24T12:49:08.668097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.stack([torch.unsqueeze(img, 0) for img in X_train], dim=0)  # Add channel dimension\n",
    "labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Convert labels to float and add a channel dimension\n",
    "model = CNN().to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "batch_size = 2  # Adjust based on your GPU memory\n",
    "train_dataset = PETDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "total_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    batch_counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        batch_counter += 1\n",
    "        print(f'Processing batch {batch_counter}/{total_batches} of epoch {epoch}')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        # Save the best model based on loss\n",
    "        if loss.item() < min(train_losses, default=float('inf')):\n",
    "            torch.save(model.state_dict(), 'data/pet_demographic/best_pet_cnn_model.pth')\n",
    "    del inputs, labels, outputs  # Clear variables to free memory\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory after each epoch\n",
    "    gc.collect()  # Collect garbage to free up memory\n",
    "\n",
    "    print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "90be1cbe60d3816f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing batch 1/30 of epoch 1\n",
      "Processing batch 2/30 of epoch 1\n",
      "Processing batch 3/30 of epoch 1\n",
      "Processing batch 4/30 of epoch 1\n",
      "Processing batch 5/30 of epoch 1\n",
      "Processing batch 6/30 of epoch 1\n",
      "Processing batch 7/30 of epoch 1\n",
      "Processing batch 8/30 of epoch 1\n",
      "Processing batch 9/30 of epoch 1\n",
      "Processing batch 10/30 of epoch 1\n",
      "Processing batch 11/30 of epoch 1\n",
      "Processing batch 12/30 of epoch 1\n",
      "Processing batch 13/30 of epoch 1\n",
      "Processing batch 14/30 of epoch 1\n",
      "Processing batch 15/30 of epoch 1\n",
      "Processing batch 16/30 of epoch 1\n",
      "Processing batch 17/30 of epoch 1\n",
      "Processing batch 18/30 of epoch 1\n",
      "Processing batch 19/30 of epoch 1\n",
      "Processing batch 20/30 of epoch 1\n",
      "Processing batch 21/30 of epoch 1\n",
      "Processing batch 22/30 of epoch 1\n",
      "Processing batch 23/30 of epoch 1\n",
      "Processing batch 24/30 of epoch 1\n",
      "Processing batch 25/30 of epoch 1\n",
      "Processing batch 26/30 of epoch 1\n",
      "Processing batch 27/30 of epoch 1\n",
      "Processing batch 28/30 of epoch 1\n",
      "Processing batch 29/30 of epoch 1\n",
      "Processing batch 30/30 of epoch 1\n",
      "Processing batch 31/30 of epoch 1\n",
      "Processing batch 32/30 of epoch 1\n",
      "Processing batch 33/30 of epoch 1\n",
      "Processing batch 34/30 of epoch 1\n",
      "Processing batch 35/30 of epoch 1\n",
      "Processing batch 36/30 of epoch 1\n",
      "Processing batch 37/30 of epoch 1\n",
      "Processing batch 38/30 of epoch 1\n",
      "Processing batch 39/30 of epoch 1\n",
      "Processing batch 40/30 of epoch 1\n",
      "Processing batch 41/30 of epoch 1\n",
      "Processing batch 42/30 of epoch 1\n",
      "Processing batch 43/30 of epoch 1\n",
      "Processing batch 44/30 of epoch 1\n",
      "Processing batch 45/30 of epoch 1\n",
      "Processing batch 46/30 of epoch 1\n",
      "Processing batch 47/30 of epoch 1\n",
      "Processing batch 48/30 of epoch 1\n",
      "Processing batch 49/30 of epoch 1\n",
      "Processing batch 50/30 of epoch 1\n",
      "Processing batch 51/30 of epoch 1\n",
      "Processing batch 52/30 of epoch 1\n",
      "Processing batch 53/30 of epoch 1\n",
      "Processing batch 54/30 of epoch 1\n",
      "Processing batch 55/30 of epoch 1\n",
      "Processing batch 56/30 of epoch 1\n",
      "Processing batch 57/30 of epoch 1\n",
      "Processing batch 58/30 of epoch 1\n",
      "Processing batch 59/30 of epoch 1\n",
      "Processing batch 60/30 of epoch 1\n",
      "Epoch [1/100], Loss: 0.0000\n",
      "Processing batch 1/30 of epoch 2\n",
      "Processing batch 2/30 of epoch 2\n",
      "Processing batch 3/30 of epoch 2\n",
      "Processing batch 4/30 of epoch 2\n",
      "Processing batch 5/30 of epoch 2\n",
      "Processing batch 6/30 of epoch 2\n",
      "Processing batch 7/30 of epoch 2\n",
      "Processing batch 8/30 of epoch 2\n",
      "Processing batch 9/30 of epoch 2\n",
      "Processing batch 10/30 of epoch 2\n",
      "Processing batch 11/30 of epoch 2\n",
      "Processing batch 12/30 of epoch 2\n",
      "Processing batch 13/30 of epoch 2\n",
      "Processing batch 14/30 of epoch 2\n",
      "Processing batch 15/30 of epoch 2\n",
      "Processing batch 16/30 of epoch 2\n",
      "Processing batch 17/30 of epoch 2\n",
      "Processing batch 18/30 of epoch 2\n",
      "Processing batch 19/30 of epoch 2\n",
      "Processing batch 20/30 of epoch 2\n",
      "Processing batch 21/30 of epoch 2\n",
      "Processing batch 22/30 of epoch 2\n",
      "Processing batch 23/30 of epoch 2\n",
      "Processing batch 24/30 of epoch 2\n",
      "Processing batch 25/30 of epoch 2\n",
      "Processing batch 26/30 of epoch 2\n",
      "Processing batch 27/30 of epoch 2\n",
      "Processing batch 28/30 of epoch 2\n",
      "Processing batch 29/30 of epoch 2\n",
      "Processing batch 30/30 of epoch 2\n",
      "Processing batch 31/30 of epoch 2\n",
      "Processing batch 32/30 of epoch 2\n",
      "Processing batch 33/30 of epoch 2\n",
      "Processing batch 34/30 of epoch 2\n",
      "Processing batch 35/30 of epoch 2\n",
      "Processing batch 36/30 of epoch 2\n",
      "Processing batch 37/30 of epoch 2\n",
      "Processing batch 38/30 of epoch 2\n",
      "Processing batch 39/30 of epoch 2\n",
      "Processing batch 40/30 of epoch 2\n",
      "Processing batch 41/30 of epoch 2\n",
      "Processing batch 42/30 of epoch 2\n",
      "Processing batch 43/30 of epoch 2\n",
      "Processing batch 44/30 of epoch 2\n",
      "Processing batch 45/30 of epoch 2\n",
      "Processing batch 46/30 of epoch 2\n",
      "Processing batch 47/30 of epoch 2\n",
      "Processing batch 48/30 of epoch 2\n",
      "Processing batch 49/30 of epoch 2\n",
      "Processing batch 50/30 of epoch 2\n",
      "Processing batch 51/30 of epoch 2\n",
      "Processing batch 52/30 of epoch 2\n",
      "Processing batch 53/30 of epoch 2\n",
      "Processing batch 54/30 of epoch 2\n",
      "Processing batch 55/30 of epoch 2\n",
      "Processing batch 56/30 of epoch 2\n",
      "Processing batch 57/30 of epoch 2\n",
      "Processing batch 58/30 of epoch 2\n",
      "Processing batch 59/30 of epoch 2\n",
      "Processing batch 60/30 of epoch 2\n",
      "Epoch [2/100], Loss: 0.0000\n",
      "Processing batch 1/30 of epoch 3\n",
      "Processing batch 2/30 of epoch 3\n",
      "Processing batch 3/30 of epoch 3\n",
      "Processing batch 4/30 of epoch 3\n",
      "Processing batch 5/30 of epoch 3\n",
      "Processing batch 6/30 of epoch 3\n",
      "Processing batch 7/30 of epoch 3\n",
      "Processing batch 8/30 of epoch 3\n",
      "Processing batch 9/30 of epoch 3\n",
      "Processing batch 10/30 of epoch 3\n",
      "Processing batch 11/30 of epoch 3\n",
      "Processing batch 12/30 of epoch 3\n",
      "Processing batch 13/30 of epoch 3\n",
      "Processing batch 14/30 of epoch 3\n",
      "Processing batch 15/30 of epoch 3\n",
      "Processing batch 16/30 of epoch 3\n",
      "Processing batch 17/30 of epoch 3\n",
      "Processing batch 18/30 of epoch 3\n",
      "Processing batch 19/30 of epoch 3\n",
      "Processing batch 20/30 of epoch 3\n",
      "Processing batch 21/30 of epoch 3\n",
      "Processing batch 22/30 of epoch 3\n",
      "Processing batch 23/30 of epoch 3\n",
      "Processing batch 24/30 of epoch 3\n",
      "Processing batch 25/30 of epoch 3\n",
      "Processing batch 26/30 of epoch 3\n",
      "Processing batch 27/30 of epoch 3\n",
      "Processing batch 28/30 of epoch 3\n",
      "Processing batch 29/30 of epoch 3\n",
      "Processing batch 30/30 of epoch 3\n",
      "Processing batch 31/30 of epoch 3\n",
      "Processing batch 32/30 of epoch 3\n",
      "Processing batch 33/30 of epoch 3\n",
      "Processing batch 34/30 of epoch 3\n",
      "Processing batch 35/30 of epoch 3\n",
      "Processing batch 36/30 of epoch 3\n",
      "Processing batch 37/30 of epoch 3\n",
      "Processing batch 38/30 of epoch 3\n",
      "Processing batch 39/30 of epoch 3\n",
      "Processing batch 40/30 of epoch 3\n",
      "Processing batch 41/30 of epoch 3\n",
      "Processing batch 42/30 of epoch 3\n",
      "Processing batch 43/30 of epoch 3\n",
      "Processing batch 44/30 of epoch 3\n",
      "Processing batch 45/30 of epoch 3\n",
      "Processing batch 46/30 of epoch 3\n",
      "Processing batch 47/30 of epoch 3\n",
      "Processing batch 48/30 of epoch 3\n",
      "Processing batch 49/30 of epoch 3\n",
      "Processing batch 50/30 of epoch 3\n",
      "Processing batch 51/30 of epoch 3\n",
      "Processing batch 52/30 of epoch 3\n",
      "Processing batch 53/30 of epoch 3\n",
      "Processing batch 54/30 of epoch 3\n",
      "Processing batch 55/30 of epoch 3\n",
      "Processing batch 56/30 of epoch 3\n",
      "Processing batch 57/30 of epoch 3\n",
      "Processing batch 58/30 of epoch 3\n",
      "Processing batch 59/30 of epoch 3\n",
      "Processing batch 60/30 of epoch 3\n",
      "Epoch [3/100], Loss: 0.0000\n",
      "Processing batch 1/30 of epoch 4\n",
      "Processing batch 2/30 of epoch 4\n",
      "Processing batch 3/30 of epoch 4\n",
      "Processing batch 4/30 of epoch 4\n",
      "Processing batch 5/30 of epoch 4\n",
      "Processing batch 6/30 of epoch 4\n",
      "Processing batch 7/30 of epoch 4\n",
      "Processing batch 8/30 of epoch 4\n",
      "Processing batch 9/30 of epoch 4\n",
      "Processing batch 10/30 of epoch 4\n",
      "Processing batch 11/30 of epoch 4\n",
      "Processing batch 12/30 of epoch 4\n",
      "Processing batch 13/30 of epoch 4\n",
      "Processing batch 14/30 of epoch 4\n",
      "Processing batch 15/30 of epoch 4\n",
      "Processing batch 16/30 of epoch 4\n",
      "Processing batch 17/30 of epoch 4\n",
      "Processing batch 18/30 of epoch 4\n",
      "Processing batch 19/30 of epoch 4\n",
      "Processing batch 20/30 of epoch 4\n",
      "Processing batch 21/30 of epoch 4\n",
      "Processing batch 22/30 of epoch 4\n",
      "Processing batch 23/30 of epoch 4\n",
      "Processing batch 24/30 of epoch 4\n",
      "Processing batch 25/30 of epoch 4\n",
      "Processing batch 26/30 of epoch 4\n",
      "Processing batch 27/30 of epoch 4\n",
      "Processing batch 28/30 of epoch 4\n",
      "Processing batch 29/30 of epoch 4\n",
      "Processing batch 30/30 of epoch 4\n",
      "Processing batch 31/30 of epoch 4\n",
      "Processing batch 32/30 of epoch 4\n",
      "Processing batch 33/30 of epoch 4\n",
      "Processing batch 34/30 of epoch 4\n",
      "Processing batch 35/30 of epoch 4\n",
      "Processing batch 36/30 of epoch 4\n",
      "Processing batch 37/30 of epoch 4\n",
      "Processing batch 38/30 of epoch 4\n",
      "Processing batch 39/30 of epoch 4\n",
      "Processing batch 40/30 of epoch 4\n",
      "Processing batch 41/30 of epoch 4\n",
      "Processing batch 42/30 of epoch 4\n",
      "Processing batch 43/30 of epoch 4\n",
      "Processing batch 44/30 of epoch 4\n",
      "Processing batch 45/30 of epoch 4\n",
      "Processing batch 46/30 of epoch 4\n",
      "Processing batch 47/30 of epoch 4\n",
      "Processing batch 48/30 of epoch 4\n",
      "Processing batch 49/30 of epoch 4\n",
      "Processing batch 50/30 of epoch 4\n",
      "Processing batch 51/30 of epoch 4\n",
      "Processing batch 52/30 of epoch 4\n",
      "Processing batch 53/30 of epoch 4\n",
      "Processing batch 54/30 of epoch 4\n",
      "Processing batch 55/30 of epoch 4\n",
      "Processing batch 56/30 of epoch 4\n",
      "Processing batch 57/30 of epoch 4\n",
      "Processing batch 58/30 of epoch 4\n",
      "Processing batch 59/30 of epoch 4\n",
      "Processing batch 60/30 of epoch 4\n",
      "Epoch [4/100], Loss: 100.0000\n",
      "Processing batch 1/30 of epoch 5\n",
      "Processing batch 2/30 of epoch 5\n",
      "Processing batch 3/30 of epoch 5\n",
      "Processing batch 4/30 of epoch 5\n",
      "Processing batch 5/30 of epoch 5\n",
      "Processing batch 6/30 of epoch 5\n",
      "Processing batch 7/30 of epoch 5\n",
      "Processing batch 8/30 of epoch 5\n",
      "Processing batch 9/30 of epoch 5\n",
      "Processing batch 10/30 of epoch 5\n",
      "Processing batch 11/30 of epoch 5\n",
      "Processing batch 12/30 of epoch 5\n",
      "Processing batch 13/30 of epoch 5\n",
      "Processing batch 14/30 of epoch 5\n",
      "Processing batch 15/30 of epoch 5\n",
      "Processing batch 16/30 of epoch 5\n",
      "Processing batch 17/30 of epoch 5\n",
      "Processing batch 18/30 of epoch 5\n",
      "Processing batch 19/30 of epoch 5\n",
      "Processing batch 20/30 of epoch 5\n",
      "Processing batch 21/30 of epoch 5\n",
      "Processing batch 22/30 of epoch 5\n",
      "Processing batch 23/30 of epoch 5\n",
      "Processing batch 24/30 of epoch 5\n",
      "Processing batch 25/30 of epoch 5\n",
      "Processing batch 26/30 of epoch 5\n",
      "Processing batch 27/30 of epoch 5\n",
      "Processing batch 28/30 of epoch 5\n",
      "Processing batch 29/30 of epoch 5\n",
      "Processing batch 30/30 of epoch 5\n",
      "Processing batch 31/30 of epoch 5\n",
      "Processing batch 32/30 of epoch 5\n",
      "Processing batch 33/30 of epoch 5\n",
      "Processing batch 34/30 of epoch 5\n",
      "Processing batch 35/30 of epoch 5\n",
      "Processing batch 36/30 of epoch 5\n",
      "Processing batch 37/30 of epoch 5\n",
      "Processing batch 38/30 of epoch 5\n",
      "Processing batch 39/30 of epoch 5\n",
      "Processing batch 40/30 of epoch 5\n",
      "Processing batch 41/30 of epoch 5\n",
      "Processing batch 42/30 of epoch 5\n",
      "Processing batch 43/30 of epoch 5\n",
      "Processing batch 44/30 of epoch 5\n",
      "Processing batch 45/30 of epoch 5\n",
      "Processing batch 46/30 of epoch 5\n",
      "Processing batch 47/30 of epoch 5\n",
      "Processing batch 48/30 of epoch 5\n",
      "Processing batch 49/30 of epoch 5\n",
      "Processing batch 50/30 of epoch 5\n",
      "Processing batch 51/30 of epoch 5\n",
      "Processing batch 52/30 of epoch 5\n",
      "Processing batch 53/30 of epoch 5\n",
      "Processing batch 54/30 of epoch 5\n",
      "Processing batch 55/30 of epoch 5\n",
      "Processing batch 56/30 of epoch 5\n",
      "Processing batch 57/30 of epoch 5\n",
      "Processing batch 58/30 of epoch 5\n",
      "Processing batch 59/30 of epoch 5\n",
      "Processing batch 60/30 of epoch 5\n",
      "Epoch [5/100], Loss: 0.0000\n",
      "Processing batch 1/30 of epoch 6\n",
      "Processing batch 2/30 of epoch 6\n",
      "Processing batch 3/30 of epoch 6\n",
      "Processing batch 4/30 of epoch 6\n",
      "Processing batch 5/30 of epoch 6\n",
      "Processing batch 6/30 of epoch 6\n",
      "Processing batch 7/30 of epoch 6\n",
      "Processing batch 8/30 of epoch 6\n",
      "Processing batch 9/30 of epoch 6\n",
      "Processing batch 10/30 of epoch 6\n",
      "Processing batch 11/30 of epoch 6\n",
      "Processing batch 12/30 of epoch 6\n",
      "Processing batch 13/30 of epoch 6\n",
      "Processing batch 14/30 of epoch 6\n",
      "Processing batch 15/30 of epoch 6\n",
      "Processing batch 16/30 of epoch 6\n",
      "Processing batch 17/30 of epoch 6\n",
      "Processing batch 18/30 of epoch 6\n",
      "Processing batch 19/30 of epoch 6\n",
      "Processing batch 20/30 of epoch 6\n",
      "Processing batch 21/30 of epoch 6\n",
      "Processing batch 22/30 of epoch 6\n",
      "Processing batch 23/30 of epoch 6\n",
      "Processing batch 24/30 of epoch 6\n",
      "Processing batch 25/30 of epoch 6\n",
      "Processing batch 26/30 of epoch 6\n",
      "Processing batch 27/30 of epoch 6\n",
      "Processing batch 28/30 of epoch 6\n",
      "Processing batch 29/30 of epoch 6\n",
      "Processing batch 30/30 of epoch 6\n",
      "Processing batch 31/30 of epoch 6\n",
      "Processing batch 32/30 of epoch 6\n",
      "Processing batch 33/30 of epoch 6\n",
      "Processing batch 34/30 of epoch 6\n",
      "Processing batch 35/30 of epoch 6\n",
      "Processing batch 36/30 of epoch 6\n",
      "Processing batch 37/30 of epoch 6\n",
      "Processing batch 38/30 of epoch 6\n",
      "Processing batch 39/30 of epoch 6\n",
      "Processing batch 40/30 of epoch 6\n",
      "Processing batch 41/30 of epoch 6\n",
      "Processing batch 42/30 of epoch 6\n",
      "Processing batch 43/30 of epoch 6\n",
      "Processing batch 44/30 of epoch 6\n",
      "Processing batch 45/30 of epoch 6\n",
      "Processing batch 46/30 of epoch 6\n",
      "Processing batch 47/30 of epoch 6\n",
      "Processing batch 48/30 of epoch 6\n",
      "Processing batch 49/30 of epoch 6\n",
      "Processing batch 50/30 of epoch 6\n",
      "Processing batch 51/30 of epoch 6\n",
      "Processing batch 52/30 of epoch 6\n",
      "Processing batch 53/30 of epoch 6\n",
      "Processing batch 54/30 of epoch 6\n",
      "Processing batch 55/30 of epoch 6\n",
      "Processing batch 56/30 of epoch 6\n",
      "Processing batch 57/30 of epoch 6\n",
      "Processing batch 58/30 of epoch 6\n",
      "Processing batch 59/30 of epoch 6\n",
      "Processing batch 60/30 of epoch 6\n",
      "Epoch [6/100], Loss: 100.0000\n",
      "Processing batch 1/30 of epoch 7\n",
      "Processing batch 2/30 of epoch 7\n",
      "Processing batch 3/30 of epoch 7\n",
      "Processing batch 4/30 of epoch 7\n",
      "Processing batch 5/30 of epoch 7\n",
      "Processing batch 6/30 of epoch 7\n",
      "Processing batch 7/30 of epoch 7\n",
      "Processing batch 8/30 of epoch 7\n",
      "Processing batch 9/30 of epoch 7\n",
      "Processing batch 10/30 of epoch 7\n",
      "Processing batch 11/30 of epoch 7\n",
      "Processing batch 12/30 of epoch 7\n",
      "Processing batch 13/30 of epoch 7\n",
      "Processing batch 14/30 of epoch 7\n",
      "Processing batch 15/30 of epoch 7\n",
      "Processing batch 16/30 of epoch 7\n",
      "Processing batch 17/30 of epoch 7\n",
      "Processing batch 18/30 of epoch 7\n",
      "Processing batch 19/30 of epoch 7\n",
      "Processing batch 20/30 of epoch 7\n",
      "Processing batch 21/30 of epoch 7\n",
      "Processing batch 22/30 of epoch 7\n",
      "Processing batch 23/30 of epoch 7\n",
      "Processing batch 24/30 of epoch 7\n",
      "Processing batch 25/30 of epoch 7\n",
      "Processing batch 26/30 of epoch 7\n",
      "Processing batch 27/30 of epoch 7\n",
      "Processing batch 28/30 of epoch 7\n",
      "Processing batch 29/30 of epoch 7\n",
      "Processing batch 30/30 of epoch 7\n",
      "Processing batch 31/30 of epoch 7\n",
      "Processing batch 32/30 of epoch 7\n",
      "Processing batch 33/30 of epoch 7\n",
      "Processing batch 34/30 of epoch 7\n",
      "Processing batch 35/30 of epoch 7\n",
      "Processing batch 36/30 of epoch 7\n",
      "Processing batch 37/30 of epoch 7\n",
      "Processing batch 38/30 of epoch 7\n",
      "Processing batch 39/30 of epoch 7\n",
      "Processing batch 40/30 of epoch 7\n",
      "Processing batch 41/30 of epoch 7\n",
      "Processing batch 42/30 of epoch 7\n",
      "Processing batch 43/30 of epoch 7\n",
      "Processing batch 44/30 of epoch 7\n",
      "Processing batch 45/30 of epoch 7\n",
      "Processing batch 46/30 of epoch 7\n",
      "Processing batch 47/30 of epoch 7\n",
      "Processing batch 48/30 of epoch 7\n",
      "Processing batch 49/30 of epoch 7\n",
      "Processing batch 50/30 of epoch 7\n",
      "Processing batch 51/30 of epoch 7\n",
      "Processing batch 52/30 of epoch 7\n",
      "Processing batch 53/30 of epoch 7\n",
      "Processing batch 54/30 of epoch 7\n",
      "Processing batch 55/30 of epoch 7\n",
      "Processing batch 56/30 of epoch 7\n",
      "Processing batch 57/30 of epoch 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     28\u001B[39m loss.backward()\n\u001B[32m     29\u001B[39m optimizer.step()\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m train_losses.append(\u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Save the best model based on loss\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m loss.item() < \u001B[38;5;28mmin\u001B[39m(train_losses, default=\u001B[38;5;28mfloat\u001B[39m(\u001B[33m'\u001B[39m\u001B[33minf\u001B[39m\u001B[33m'\u001B[39m)):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), 'data/pet_demographic/pet_cnn_model.pth')",
   "id": "d7f46236ad2b8b9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T18:04:37.991351Z",
     "start_time": "2025-07-24T17:56:43.615009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the train set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_dataset = PETDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_train = []\n",
    "    y_true_train = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_train.extend(outputs.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "    y_pred_train = (torch.tensor(y_pred_train) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_train = torch.tensor(y_true_train).numpy()\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}, Train F1 Score: {train_f1:.4f}, Train ROC AUC: {train_roc_auc:.4f}')"
   ],
   "id": "2d3299edd155a6d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.4202, Train F1 Score: 0.5917, Train ROC AUC: 0.5000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T17:55:35.349957Z",
     "start_time": "2025-07-24T17:53:33.534742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    test_dataset = PETDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_test.extend(outputs.cpu().numpy())\n",
    "        y_true_test.extend(labels.cpu().numpy())\n",
    "    y_pred_test = (torch.tensor(y_pred_test) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_test = torch.tensor(y_true_test).numpy()\n",
    "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}')"
   ],
   "id": "a05c40a6233b6a4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv Khanna\\AppData\\Local\\Temp\\ipykernel_25916\\3200254905.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  y_pred_test = (torch.tensor(y_pred_test) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4333, Test F1 Score: 0.6047, Test ROC AUC: 0.5000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:52:14.742341Z",
     "start_time": "2025-07-28T09:52:14.712699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepPETModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepPETModel, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def inheritance_test(self):\n",
    "        print(\"DeepPETModel inheritance test passed!\")\n",
    "\n",
    "    def input_gradient_hook(self, gradients):\n",
    "        \"\"\"\n",
    "        cature gradient with respect to input\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"triggered input gradient hook\")\n",
    "        self.input_gradient = gradients\n",
    "\n",
    "    def activation_gradient_hook(self, gradients):\n",
    "        \"\"\"\n",
    "        capture gradient with respect to activation maps\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"triggered activation gradient hook\")\n",
    "        self.activation_gradients = gradients\n",
    "\n",
    "    def get_activation_maps(self, x):\n",
    "        \"\"\"\n",
    "        return the activation maps of the last convolutional block\n",
    "        \"\"\"\n",
    "\n",
    "        return self.activation_maps\n",
    "\n",
    "    def get_input_gradient(self):\n",
    "        \"\"\"\n",
    "        retrieve gradient with respect to input\n",
    "        \"\"\"\n",
    "\n",
    "        return self.input_gradient\n",
    "\n",
    "    def get_activation_gradients(self):\n",
    "        \"\"\"\n",
    "        retrieve gradient with respect to activation maps\n",
    "        \"\"\"\n",
    "\n",
    "        return self.activation_gradients\n",
    "\n",
    "\n",
    "class PreActivationResBlock(torch.nn.Module):\n",
    "    def __init__(self, planes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm3d(planes)\n",
    "        self.bn2 = torch.nn.BatchNorm3d(planes)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.dropout = torch.nn.Dropout3d(p=0.25)\n",
    "\n",
    "    def preresidual_gradient_hook(self, gradients):\n",
    "        \"\"\"\n",
    "        cature gradient with respect to input\n",
    "        \"\"\"\n",
    "        self.preresidual_gradients = gradients\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if (not self.training) and (x.requires_grad):\n",
    "            # print(f\"PreActivationResBlock: triggered gradient hook\")\n",
    "            h = x.register_hook(self.preresidual_gradient_hook)\n",
    "        self.preresidual_activation_maps = out.detach().clone()\n",
    "        out += x\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepPETEncoder(DeepPETModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer0 = self._make_layer(1, 8, stride=1)\n",
    "        self.layer1 = self._make_layer(8, 16, stride=2)\n",
    "        self.layer2 = self._make_layer(16, 32, stride=2)\n",
    "        self.layer3 = self._make_layer(32, 64, stride=2)\n",
    "        self.layer4 = self._make_layer(64, 128, stride=2)\n",
    "\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.50),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.layers = [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n",
    "\n",
    "        self.input_gradient = None\n",
    "        self.gradients = None\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, stride=1):\n",
    "\n",
    "        layers = [\n",
    "            torch.nn.Conv3d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            )\n",
    "        ]\n",
    "        layers.append(PreActivationResBlock(planes=out_planes))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if (not self.training) and (x.requires_grad):\n",
    "            h = x.register_hook(self.input_gradient_hook)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        if (not self.training) and (x.requires_grad):\n",
    "            # print(f\"DeepPETEncoder: triggered gradient hook\")\n",
    "            h = x.register_hook(self.activation_gradient_hook)\n",
    "\n",
    "        # global average pooling 3d\n",
    "        x = x.mean(dim=(-3, -2, -1))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PreActivationResBlockGradCAM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GradCAM-compatible PreActivaitonResBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, planes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm3d(planes)\n",
    "        self.bn2 = torch.nn.BatchNorm3d(planes)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.dropout = torch.nn.Dropout3d(p=0.25)\n",
    "\n",
    "    def preresidual_gradient_hook(self, gradients):\n",
    "        \"\"\"\n",
    "        cature gradient with respect to input\n",
    "        \"\"\"\n",
    "        self.preresidual_gradients = gradients\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if (not self.training) and (x.requires_grad):\n",
    "            # print(f\"PreActivationResBlock: triggered gradient hook\")\n",
    "            h = x.register_hook(self.preresidual_gradient_hook)\n",
    "        self.preresidual_activation_maps = out.detach().clone()\n",
    "        out += x\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepPETEncoderGradCAM(DeepPETModel):\n",
    "    \"\"\"\n",
    "    GradCAM-compatible DeepPETEncoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer0 = self._make_conv_layer(1, 8, stride=1)\n",
    "        self.conv_layer1 = self._make_conv_layer(8, 16, stride=2)\n",
    "        self.conv_layer2 = self._make_conv_layer(16, 32, stride=2)\n",
    "        self.conv_layer3 = self._make_conv_layer(32, 64, stride=2)\n",
    "        self.conv_layer4 = self._make_conv_layer(64, 128, stride=2)\n",
    "\n",
    "        self.preres_block0 = self._make_preres_block(8)\n",
    "        self.preres_block1 = self._make_preres_block(16)\n",
    "        self.preres_block2 = self._make_preres_block(32)\n",
    "        self.preres_block3 = self._make_preres_block(64)\n",
    "        self.preres_block4 = self._make_preres_block(128)\n",
    "\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.50),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.input_gradient = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # # Freeze the encoder layers and add a classifier head\n",
    "        # for param in self.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.classifier_head = torch.nn.Sigmoid()\n",
    "\n",
    "    def _make_conv_layer(self, in_planes, out_planes, stride=1):\n",
    "\n",
    "        return torch.nn.Conv3d(\n",
    "            in_planes,\n",
    "            out_planes,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def _make_preres_block(self, planes):\n",
    "\n",
    "        return PreActivationResBlock(planes=planes)\n",
    "\n",
    "    def forward(self, x0):\n",
    "\n",
    "        if (not self.training) and (x0.requires_grad):\n",
    "            h = x0.register_hook(self.input_gradient_hook)\n",
    "\n",
    "        x0 = self.conv_layer0(x0)\n",
    "        x1 = self.preres_block0(x0)\n",
    "        x1 = x1.add(x0)\n",
    "\n",
    "        x1 = self.conv_layer1(x1)\n",
    "        x2 = self.preres_block1(x1)\n",
    "        x2 = x2.add(x1)\n",
    "\n",
    "        x2 = self.conv_layer2(x2)\n",
    "        x3 = self.preres_block2(x2)\n",
    "        x3 = x3.add(x2)\n",
    "\n",
    "        x3 = self.conv_layer3(x3)\n",
    "        x4 = self.preres_block3(x3)\n",
    "        x4 = x4.add(x3)\n",
    "\n",
    "        x4 = self.conv_layer4(x4)\n",
    "        x5 = self.preres_block4(x4)\n",
    "        if (not self.training) and (x5.requires_grad):\n",
    "            # print(f\"DeepPETEncoder: triggered gradient hook\")\n",
    "            h = x5.register_hook(self.activation_gradient_hook)\n",
    "        # make a copy of tensor and store as activation maps\n",
    "        self.activation_maps = x5.detach().clone()\n",
    "        x5 = x5.add(x4)\n",
    "\n",
    "        # global average pooling 3d\n",
    "        x5 = x5.mean(dim=(-3, -2, -1))\n",
    "        x5 = self.output(x5)\n",
    "\n",
    "        # return x5\n",
    "        return self.classifier_head(x5)\n",
    "\n",
    "\n",
    "class _DenseLayer(torch.nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.add_module(\"norm1\", torch.nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module(\"relu1\", torch.nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv1\",\n",
    "            torch.nn.Conv3d(\n",
    "                num_input_features,\n",
    "                bn_size * growth_rate,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"norm2\", torch.nn.BatchNorm3d(bn_size * growth_rate))\n",
    "        self.add_module(\"relu2\", torch.nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv2\",\n",
    "            torch.nn.Conv3d(\n",
    "                bn_size * growth_rate,\n",
    "                growth_rate,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super().forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = torch.nn.functional.dropout(\n",
    "                new_features, p=self.drop_rate, training=self.training\n",
    "            )\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(torch.nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate\n",
    "            )\n",
    "            self.add_module(f\"denselayer{(i + 1)}\", layer)\n",
    "\n",
    "\n",
    "class _Transition(torch.nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        self.add_module(\"norm\", torch.nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            \"conv\",\n",
    "            torch.nn.Conv3d(\n",
    "                num_input_features,\n",
    "                num_output_features,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\"pool\", torch.nn.AvgPool3d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DeepPETDenseNetClassifier(torch.nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_max_pool=False,\n",
    "        growth_rate=16,\n",
    "        block_config=(3, 3, 3, 3),\n",
    "        bn_size=4,\n",
    "        drop_rate=0.25,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = [\n",
    "            (\n",
    "                \"conv1\",\n",
    "                torch.nn.Conv3d(\n",
    "                    1,\n",
    "                    8,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    padding=2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ),\n",
    "            (\"norm1\", torch.nn.BatchNorm3d(8)),\n",
    "            (\"relu1\", torch.nn.ReLU(inplace=True)),\n",
    "        ]\n",
    "        if not no_max_pool:\n",
    "            self.features.append(\n",
    "                (\"pool1\", torch.nn.MaxPool3d(kernel_size=3, stride=2, padding=1))\n",
    "            )\n",
    "        self.features = torch.nn.Sequential(OrderedDict(self.features))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = 8\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.features.add_module(f\"denseblock{(i + 1)}\", block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(\n",
    "                    num_input_features=num_features,\n",
    "                    num_output_features=num_features // 2,\n",
    "                )\n",
    "                self.features.add_module(f\"transition{(i + 1)}\", trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # final batch norm\n",
    "        self.features.add_module(\"norm5\", torch.nn.BatchNorm3d(num_features))\n",
    "        # final dense layer\n",
    "        self.classifier = torch.nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.nn.functional.adaptive_avg_pool3d(features, output_size=(1, 1, 1)).view(\n",
    "            features.size(0), -1\n",
    "        )\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ],
   "id": "ca48807871acae24",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:52:16.742028Z",
     "start_time": "2025-07-28T09:52:16.578088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 2\n",
    "model = DeepPETEncoderGradCAM()\n",
    "model.load_state_dict(torch.load('weights/deeppet.pth', weights_only=True)['model'])\n",
    "model.to(device)"
   ],
   "id": "bff4f34cceaf06e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepPETEncoderGradCAM(\n",
       "  (conv_layer0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  (conv_layer1): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (conv_layer2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (conv_layer3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (conv_layer4): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (preres_block0): PreActivationResBlock(\n",
       "    (conv1): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (preres_block1): PreActivationResBlock(\n",
       "    (conv1): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (preres_block2): PreActivationResBlock(\n",
       "    (conv1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (preres_block3): PreActivationResBlock(\n",
       "    (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (preres_block4): PreActivationResBlock(\n",
       "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (dropout): Dropout3d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (classifier_head): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the pretrained ResNet before tuning",
   "id": "650a6d881a56d142"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:49:49.162119Z",
     "start_time": "2025-07-28T09:49:38.880640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the train set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_dataset = PETDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_train = []\n",
    "    y_true_train = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_train.extend(outputs.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "    y_pred_train = (torch.tensor(\n",
    "        y_pred_train) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_train = torch.tensor(y_true_train).numpy()\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "    print(\n",
    "        f'Train Accuracy: {train_accuracy:.4f}, Train F1 Score: {train_f1:.4f}, Train ROC AUC: {train_roc_auc:.4f}')"
   ],
   "id": "476bfb3805631a02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.5042, Train F1 Score: 0.6093, Train ROC AUC: 0.5614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv Khanna\\AppData\\Local\\Temp\\ipykernel_23064\\3392771040.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  y_pred_train = (torch.tensor(\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:49:54.182937Z",
     "start_time": "2025-07-28T09:49:51.631374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    test_dataset = PETDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_test.extend(outputs.cpu().numpy())\n",
    "        y_true_test.extend(labels.cpu().numpy())\n",
    "    y_pred_test = (torch.tensor(\n",
    "        y_pred_test) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_test = torch.tensor(y_true_test).numpy()\n",
    "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    print(\n",
    "        f'Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}')"
   ],
   "id": "bced0cf00832e52e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5333, Test F1 Score: 0.6500, Test ROC AUC: 0.5882\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tune the pretrained ResNet",
   "id": "8ca99e05f50cf248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-28T10:05:25.770991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.stack([torch.unsqueeze(img, 0) for img in X_train], dim=0)  # Add channel dimension\n",
    "labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Convert labels to float and add a channel dimension\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "batch_size = 2  # Adjust based on your GPU memory\n",
    "train_dataset = PETDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "total_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    batch_counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        batch_counter += 1\n",
    "        print(f'Processing batch {batch_counter}/{total_batches} of epoch {epoch}')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(f'Output shape: {outputs.shape}, Labels shape: {labels.shape}')\n",
    "        # print(f'Output: {outputs[:5]}, Labels: {labels[:5]}')\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        # Save the best model based on loss\n",
    "        if loss.item() < min(train_losses, default=float('inf')):\n",
    "            torch.save(model.state_dict(), 'weights/best_deeppet_tuned.pth')\n",
    "    del inputs, labels, outputs  # Clear variables to free memory\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory after each epoch\n",
    "    gc.collect()  # Collect garbage to free up memory\n",
    "\n",
    "    print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'weights/deeppet_tuned.pth')"
   ],
   "id": "898902d2c6aec47e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/60 of epoch 1\n",
      "Processing batch 2/60 of epoch 1\n",
      "Processing batch 3/60 of epoch 1\n",
      "Processing batch 4/60 of epoch 1\n",
      "Processing batch 5/60 of epoch 1\n",
      "Processing batch 6/60 of epoch 1\n",
      "Processing batch 7/60 of epoch 1\n",
      "Processing batch 8/60 of epoch 1\n",
      "Processing batch 9/60 of epoch 1\n",
      "Processing batch 10/60 of epoch 1\n",
      "Processing batch 11/60 of epoch 1\n",
      "Processing batch 12/60 of epoch 1\n",
      "Processing batch 13/60 of epoch 1\n",
      "Processing batch 14/60 of epoch 1\n",
      "Processing batch 15/60 of epoch 1\n",
      "Processing batch 16/60 of epoch 1\n",
      "Processing batch 17/60 of epoch 1\n",
      "Processing batch 18/60 of epoch 1\n",
      "Processing batch 19/60 of epoch 1\n",
      "Processing batch 20/60 of epoch 1\n",
      "Processing batch 21/60 of epoch 1\n",
      "Processing batch 22/60 of epoch 1\n",
      "Processing batch 23/60 of epoch 1\n",
      "Processing batch 24/60 of epoch 1\n",
      "Processing batch 25/60 of epoch 1\n",
      "Processing batch 26/60 of epoch 1\n",
      "Processing batch 27/60 of epoch 1\n",
      "Processing batch 28/60 of epoch 1\n",
      "Processing batch 29/60 of epoch 1\n",
      "Processing batch 30/60 of epoch 1\n",
      "Processing batch 31/60 of epoch 1\n",
      "Processing batch 32/60 of epoch 1\n",
      "Processing batch 33/60 of epoch 1\n",
      "Processing batch 34/60 of epoch 1\n",
      "Processing batch 35/60 of epoch 1\n",
      "Processing batch 36/60 of epoch 1\n",
      "Processing batch 37/60 of epoch 1\n",
      "Processing batch 38/60 of epoch 1\n",
      "Processing batch 39/60 of epoch 1\n",
      "Processing batch 40/60 of epoch 1\n",
      "Processing batch 41/60 of epoch 1\n",
      "Processing batch 42/60 of epoch 1\n",
      "Processing batch 43/60 of epoch 1\n",
      "Processing batch 44/60 of epoch 1\n",
      "Processing batch 45/60 of epoch 1\n",
      "Processing batch 46/60 of epoch 1\n",
      "Processing batch 47/60 of epoch 1\n",
      "Processing batch 48/60 of epoch 1\n",
      "Processing batch 49/60 of epoch 1\n",
      "Processing batch 50/60 of epoch 1\n",
      "Processing batch 51/60 of epoch 1\n",
      "Processing batch 52/60 of epoch 1\n",
      "Processing batch 53/60 of epoch 1\n",
      "Processing batch 54/60 of epoch 1\n",
      "Processing batch 55/60 of epoch 1\n",
      "Processing batch 56/60 of epoch 1\n",
      "Processing batch 57/60 of epoch 1\n",
      "Processing batch 58/60 of epoch 1\n",
      "Processing batch 59/60 of epoch 1\n",
      "Processing batch 60/60 of epoch 1\n",
      "Epoch [1/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 2\n",
      "Processing batch 2/60 of epoch 2\n",
      "Processing batch 3/60 of epoch 2\n",
      "Processing batch 4/60 of epoch 2\n",
      "Processing batch 5/60 of epoch 2\n",
      "Processing batch 6/60 of epoch 2\n",
      "Processing batch 7/60 of epoch 2\n",
      "Processing batch 8/60 of epoch 2\n",
      "Processing batch 9/60 of epoch 2\n",
      "Processing batch 10/60 of epoch 2\n",
      "Processing batch 11/60 of epoch 2\n",
      "Processing batch 12/60 of epoch 2\n",
      "Processing batch 13/60 of epoch 2\n",
      "Processing batch 14/60 of epoch 2\n",
      "Processing batch 15/60 of epoch 2\n",
      "Processing batch 16/60 of epoch 2\n",
      "Processing batch 17/60 of epoch 2\n",
      "Processing batch 18/60 of epoch 2\n",
      "Processing batch 19/60 of epoch 2\n",
      "Processing batch 20/60 of epoch 2\n",
      "Processing batch 21/60 of epoch 2\n",
      "Processing batch 22/60 of epoch 2\n",
      "Processing batch 23/60 of epoch 2\n",
      "Processing batch 24/60 of epoch 2\n",
      "Processing batch 25/60 of epoch 2\n",
      "Processing batch 26/60 of epoch 2\n",
      "Processing batch 27/60 of epoch 2\n",
      "Processing batch 28/60 of epoch 2\n",
      "Processing batch 29/60 of epoch 2\n",
      "Processing batch 30/60 of epoch 2\n",
      "Processing batch 31/60 of epoch 2\n",
      "Processing batch 32/60 of epoch 2\n",
      "Processing batch 33/60 of epoch 2\n",
      "Processing batch 34/60 of epoch 2\n",
      "Processing batch 35/60 of epoch 2\n",
      "Processing batch 36/60 of epoch 2\n",
      "Processing batch 37/60 of epoch 2\n",
      "Processing batch 38/60 of epoch 2\n",
      "Processing batch 39/60 of epoch 2\n",
      "Processing batch 40/60 of epoch 2\n",
      "Processing batch 41/60 of epoch 2\n",
      "Processing batch 42/60 of epoch 2\n",
      "Processing batch 43/60 of epoch 2\n",
      "Processing batch 44/60 of epoch 2\n",
      "Processing batch 45/60 of epoch 2\n",
      "Processing batch 46/60 of epoch 2\n",
      "Processing batch 47/60 of epoch 2\n",
      "Processing batch 48/60 of epoch 2\n",
      "Processing batch 49/60 of epoch 2\n",
      "Processing batch 50/60 of epoch 2\n",
      "Processing batch 51/60 of epoch 2\n",
      "Processing batch 52/60 of epoch 2\n",
      "Processing batch 53/60 of epoch 2\n",
      "Processing batch 54/60 of epoch 2\n",
      "Processing batch 55/60 of epoch 2\n",
      "Processing batch 56/60 of epoch 2\n",
      "Processing batch 57/60 of epoch 2\n",
      "Processing batch 58/60 of epoch 2\n",
      "Processing batch 59/60 of epoch 2\n",
      "Processing batch 60/60 of epoch 2\n",
      "Epoch [2/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 3\n",
      "Processing batch 2/60 of epoch 3\n",
      "Processing batch 3/60 of epoch 3\n",
      "Processing batch 4/60 of epoch 3\n",
      "Processing batch 5/60 of epoch 3\n",
      "Processing batch 6/60 of epoch 3\n",
      "Processing batch 7/60 of epoch 3\n",
      "Processing batch 8/60 of epoch 3\n",
      "Processing batch 9/60 of epoch 3\n",
      "Processing batch 10/60 of epoch 3\n",
      "Processing batch 11/60 of epoch 3\n",
      "Processing batch 12/60 of epoch 3\n",
      "Processing batch 13/60 of epoch 3\n",
      "Processing batch 14/60 of epoch 3\n",
      "Processing batch 15/60 of epoch 3\n",
      "Processing batch 16/60 of epoch 3\n",
      "Processing batch 17/60 of epoch 3\n",
      "Processing batch 18/60 of epoch 3\n",
      "Processing batch 19/60 of epoch 3\n",
      "Processing batch 20/60 of epoch 3\n",
      "Processing batch 21/60 of epoch 3\n",
      "Processing batch 22/60 of epoch 3\n",
      "Processing batch 23/60 of epoch 3\n",
      "Processing batch 24/60 of epoch 3\n",
      "Processing batch 25/60 of epoch 3\n",
      "Processing batch 26/60 of epoch 3\n",
      "Processing batch 27/60 of epoch 3\n",
      "Processing batch 28/60 of epoch 3\n",
      "Processing batch 29/60 of epoch 3\n",
      "Processing batch 30/60 of epoch 3\n",
      "Processing batch 31/60 of epoch 3\n",
      "Processing batch 32/60 of epoch 3\n",
      "Processing batch 33/60 of epoch 3\n",
      "Processing batch 34/60 of epoch 3\n",
      "Processing batch 35/60 of epoch 3\n",
      "Processing batch 36/60 of epoch 3\n",
      "Processing batch 37/60 of epoch 3\n",
      "Processing batch 38/60 of epoch 3\n",
      "Processing batch 39/60 of epoch 3\n",
      "Processing batch 40/60 of epoch 3\n",
      "Processing batch 41/60 of epoch 3\n",
      "Processing batch 42/60 of epoch 3\n",
      "Processing batch 43/60 of epoch 3\n",
      "Processing batch 44/60 of epoch 3\n",
      "Processing batch 45/60 of epoch 3\n",
      "Processing batch 46/60 of epoch 3\n",
      "Processing batch 47/60 of epoch 3\n",
      "Processing batch 48/60 of epoch 3\n",
      "Processing batch 49/60 of epoch 3\n",
      "Processing batch 50/60 of epoch 3\n",
      "Processing batch 51/60 of epoch 3\n",
      "Processing batch 52/60 of epoch 3\n",
      "Processing batch 53/60 of epoch 3\n",
      "Processing batch 54/60 of epoch 3\n",
      "Processing batch 55/60 of epoch 3\n",
      "Processing batch 56/60 of epoch 3\n",
      "Processing batch 57/60 of epoch 3\n",
      "Processing batch 58/60 of epoch 3\n",
      "Processing batch 59/60 of epoch 3\n",
      "Processing batch 60/60 of epoch 3\n",
      "Epoch [3/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 4\n",
      "Processing batch 2/60 of epoch 4\n",
      "Processing batch 3/60 of epoch 4\n",
      "Processing batch 4/60 of epoch 4\n",
      "Processing batch 5/60 of epoch 4\n",
      "Processing batch 6/60 of epoch 4\n",
      "Processing batch 7/60 of epoch 4\n",
      "Processing batch 8/60 of epoch 4\n",
      "Processing batch 9/60 of epoch 4\n",
      "Processing batch 10/60 of epoch 4\n",
      "Processing batch 11/60 of epoch 4\n",
      "Processing batch 12/60 of epoch 4\n",
      "Processing batch 13/60 of epoch 4\n",
      "Processing batch 14/60 of epoch 4\n",
      "Processing batch 15/60 of epoch 4\n",
      "Processing batch 16/60 of epoch 4\n",
      "Processing batch 17/60 of epoch 4\n",
      "Processing batch 18/60 of epoch 4\n",
      "Processing batch 19/60 of epoch 4\n",
      "Processing batch 20/60 of epoch 4\n",
      "Processing batch 21/60 of epoch 4\n",
      "Processing batch 22/60 of epoch 4\n",
      "Processing batch 23/60 of epoch 4\n",
      "Processing batch 24/60 of epoch 4\n",
      "Processing batch 25/60 of epoch 4\n",
      "Processing batch 26/60 of epoch 4\n",
      "Processing batch 27/60 of epoch 4\n",
      "Processing batch 28/60 of epoch 4\n",
      "Processing batch 29/60 of epoch 4\n",
      "Processing batch 30/60 of epoch 4\n",
      "Processing batch 31/60 of epoch 4\n",
      "Processing batch 32/60 of epoch 4\n",
      "Processing batch 33/60 of epoch 4\n",
      "Processing batch 34/60 of epoch 4\n",
      "Processing batch 35/60 of epoch 4\n",
      "Processing batch 36/60 of epoch 4\n",
      "Processing batch 37/60 of epoch 4\n",
      "Processing batch 38/60 of epoch 4\n",
      "Processing batch 39/60 of epoch 4\n",
      "Processing batch 40/60 of epoch 4\n",
      "Processing batch 41/60 of epoch 4\n",
      "Processing batch 42/60 of epoch 4\n",
      "Processing batch 43/60 of epoch 4\n",
      "Processing batch 44/60 of epoch 4\n",
      "Processing batch 45/60 of epoch 4\n",
      "Processing batch 46/60 of epoch 4\n",
      "Processing batch 47/60 of epoch 4\n",
      "Processing batch 48/60 of epoch 4\n",
      "Processing batch 49/60 of epoch 4\n",
      "Processing batch 50/60 of epoch 4\n",
      "Processing batch 51/60 of epoch 4\n",
      "Processing batch 52/60 of epoch 4\n",
      "Processing batch 53/60 of epoch 4\n",
      "Processing batch 54/60 of epoch 4\n",
      "Processing batch 55/60 of epoch 4\n",
      "Processing batch 56/60 of epoch 4\n",
      "Processing batch 57/60 of epoch 4\n",
      "Processing batch 58/60 of epoch 4\n",
      "Processing batch 59/60 of epoch 4\n",
      "Processing batch 60/60 of epoch 4\n",
      "Epoch [4/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 5\n",
      "Processing batch 2/60 of epoch 5\n",
      "Processing batch 3/60 of epoch 5\n",
      "Processing batch 4/60 of epoch 5\n",
      "Processing batch 5/60 of epoch 5\n",
      "Processing batch 6/60 of epoch 5\n",
      "Processing batch 7/60 of epoch 5\n",
      "Processing batch 8/60 of epoch 5\n",
      "Processing batch 9/60 of epoch 5\n",
      "Processing batch 10/60 of epoch 5\n",
      "Processing batch 11/60 of epoch 5\n",
      "Processing batch 12/60 of epoch 5\n",
      "Processing batch 13/60 of epoch 5\n",
      "Processing batch 14/60 of epoch 5\n",
      "Processing batch 15/60 of epoch 5\n",
      "Processing batch 16/60 of epoch 5\n",
      "Processing batch 17/60 of epoch 5\n",
      "Processing batch 18/60 of epoch 5\n",
      "Processing batch 19/60 of epoch 5\n",
      "Processing batch 20/60 of epoch 5\n",
      "Processing batch 21/60 of epoch 5\n",
      "Processing batch 22/60 of epoch 5\n",
      "Processing batch 23/60 of epoch 5\n",
      "Processing batch 24/60 of epoch 5\n",
      "Processing batch 25/60 of epoch 5\n",
      "Processing batch 26/60 of epoch 5\n",
      "Processing batch 27/60 of epoch 5\n",
      "Processing batch 28/60 of epoch 5\n",
      "Processing batch 29/60 of epoch 5\n",
      "Processing batch 30/60 of epoch 5\n",
      "Processing batch 31/60 of epoch 5\n",
      "Processing batch 32/60 of epoch 5\n",
      "Processing batch 33/60 of epoch 5\n",
      "Processing batch 34/60 of epoch 5\n",
      "Processing batch 35/60 of epoch 5\n",
      "Processing batch 36/60 of epoch 5\n",
      "Processing batch 37/60 of epoch 5\n",
      "Processing batch 38/60 of epoch 5\n",
      "Processing batch 39/60 of epoch 5\n",
      "Processing batch 40/60 of epoch 5\n",
      "Processing batch 41/60 of epoch 5\n",
      "Processing batch 42/60 of epoch 5\n",
      "Processing batch 43/60 of epoch 5\n",
      "Processing batch 44/60 of epoch 5\n",
      "Processing batch 45/60 of epoch 5\n",
      "Processing batch 46/60 of epoch 5\n",
      "Processing batch 47/60 of epoch 5\n",
      "Processing batch 48/60 of epoch 5\n",
      "Processing batch 49/60 of epoch 5\n",
      "Processing batch 50/60 of epoch 5\n",
      "Processing batch 51/60 of epoch 5\n",
      "Processing batch 52/60 of epoch 5\n",
      "Processing batch 53/60 of epoch 5\n",
      "Processing batch 54/60 of epoch 5\n",
      "Processing batch 55/60 of epoch 5\n",
      "Processing batch 56/60 of epoch 5\n",
      "Processing batch 57/60 of epoch 5\n",
      "Processing batch 58/60 of epoch 5\n",
      "Processing batch 59/60 of epoch 5\n",
      "Processing batch 60/60 of epoch 5\n",
      "Epoch [5/100], Loss: 0.0000\n",
      "Processing batch 1/60 of epoch 6\n",
      "Processing batch 2/60 of epoch 6\n",
      "Processing batch 3/60 of epoch 6\n",
      "Processing batch 4/60 of epoch 6\n",
      "Processing batch 5/60 of epoch 6\n",
      "Processing batch 6/60 of epoch 6\n",
      "Processing batch 7/60 of epoch 6\n",
      "Processing batch 8/60 of epoch 6\n",
      "Processing batch 9/60 of epoch 6\n",
      "Processing batch 10/60 of epoch 6\n",
      "Processing batch 11/60 of epoch 6\n",
      "Processing batch 12/60 of epoch 6\n",
      "Processing batch 13/60 of epoch 6\n",
      "Processing batch 14/60 of epoch 6\n",
      "Processing batch 15/60 of epoch 6\n",
      "Processing batch 16/60 of epoch 6\n",
      "Processing batch 17/60 of epoch 6\n",
      "Processing batch 18/60 of epoch 6\n",
      "Processing batch 19/60 of epoch 6\n",
      "Processing batch 20/60 of epoch 6\n",
      "Processing batch 21/60 of epoch 6\n",
      "Processing batch 22/60 of epoch 6\n",
      "Processing batch 23/60 of epoch 6\n",
      "Processing batch 24/60 of epoch 6\n",
      "Processing batch 25/60 of epoch 6\n",
      "Processing batch 26/60 of epoch 6\n",
      "Processing batch 27/60 of epoch 6\n",
      "Processing batch 28/60 of epoch 6\n",
      "Processing batch 29/60 of epoch 6\n",
      "Processing batch 30/60 of epoch 6\n",
      "Processing batch 31/60 of epoch 6\n",
      "Processing batch 32/60 of epoch 6\n",
      "Processing batch 33/60 of epoch 6\n",
      "Processing batch 34/60 of epoch 6\n",
      "Processing batch 35/60 of epoch 6\n",
      "Processing batch 36/60 of epoch 6\n",
      "Processing batch 37/60 of epoch 6\n",
      "Processing batch 38/60 of epoch 6\n",
      "Processing batch 39/60 of epoch 6\n",
      "Processing batch 40/60 of epoch 6\n",
      "Processing batch 41/60 of epoch 6\n",
      "Processing batch 42/60 of epoch 6\n",
      "Processing batch 43/60 of epoch 6\n",
      "Processing batch 44/60 of epoch 6\n",
      "Processing batch 45/60 of epoch 6\n",
      "Processing batch 46/60 of epoch 6\n",
      "Processing batch 47/60 of epoch 6\n",
      "Processing batch 48/60 of epoch 6\n",
      "Processing batch 49/60 of epoch 6\n",
      "Processing batch 50/60 of epoch 6\n",
      "Processing batch 51/60 of epoch 6\n",
      "Processing batch 52/60 of epoch 6\n",
      "Processing batch 53/60 of epoch 6\n",
      "Processing batch 54/60 of epoch 6\n",
      "Processing batch 55/60 of epoch 6\n",
      "Processing batch 56/60 of epoch 6\n",
      "Processing batch 57/60 of epoch 6\n",
      "Processing batch 58/60 of epoch 6\n",
      "Processing batch 59/60 of epoch 6\n",
      "Processing batch 60/60 of epoch 6\n",
      "Epoch [6/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 7\n",
      "Processing batch 2/60 of epoch 7\n",
      "Processing batch 3/60 of epoch 7\n",
      "Processing batch 4/60 of epoch 7\n",
      "Processing batch 5/60 of epoch 7\n",
      "Processing batch 6/60 of epoch 7\n",
      "Processing batch 7/60 of epoch 7\n",
      "Processing batch 8/60 of epoch 7\n",
      "Processing batch 9/60 of epoch 7\n",
      "Processing batch 10/60 of epoch 7\n",
      "Processing batch 11/60 of epoch 7\n",
      "Processing batch 12/60 of epoch 7\n",
      "Processing batch 13/60 of epoch 7\n",
      "Processing batch 14/60 of epoch 7\n",
      "Processing batch 15/60 of epoch 7\n",
      "Processing batch 16/60 of epoch 7\n",
      "Processing batch 17/60 of epoch 7\n",
      "Processing batch 18/60 of epoch 7\n",
      "Processing batch 19/60 of epoch 7\n",
      "Processing batch 20/60 of epoch 7\n",
      "Processing batch 21/60 of epoch 7\n",
      "Processing batch 22/60 of epoch 7\n",
      "Processing batch 23/60 of epoch 7\n",
      "Processing batch 24/60 of epoch 7\n",
      "Processing batch 25/60 of epoch 7\n",
      "Processing batch 26/60 of epoch 7\n",
      "Processing batch 27/60 of epoch 7\n",
      "Processing batch 28/60 of epoch 7\n",
      "Processing batch 29/60 of epoch 7\n",
      "Processing batch 30/60 of epoch 7\n",
      "Processing batch 31/60 of epoch 7\n",
      "Processing batch 32/60 of epoch 7\n",
      "Processing batch 33/60 of epoch 7\n",
      "Processing batch 34/60 of epoch 7\n",
      "Processing batch 35/60 of epoch 7\n",
      "Processing batch 36/60 of epoch 7\n",
      "Processing batch 37/60 of epoch 7\n",
      "Processing batch 38/60 of epoch 7\n",
      "Processing batch 39/60 of epoch 7\n",
      "Processing batch 40/60 of epoch 7\n",
      "Processing batch 41/60 of epoch 7\n",
      "Processing batch 42/60 of epoch 7\n",
      "Processing batch 43/60 of epoch 7\n",
      "Processing batch 44/60 of epoch 7\n",
      "Processing batch 45/60 of epoch 7\n",
      "Processing batch 46/60 of epoch 7\n",
      "Processing batch 47/60 of epoch 7\n",
      "Processing batch 48/60 of epoch 7\n",
      "Processing batch 57/60 of epoch 14\n",
      "Processing batch 58/60 of epoch 14\n",
      "Processing batch 59/60 of epoch 14\n",
      "Processing batch 60/60 of epoch 14\n",
      "Epoch [14/100], Loss: 0.0000\n",
      "Processing batch 1/60 of epoch 15\n",
      "Processing batch 2/60 of epoch 15\n",
      "Processing batch 3/60 of epoch 15\n",
      "Processing batch 4/60 of epoch 15\n",
      "Processing batch 5/60 of epoch 15\n",
      "Processing batch 6/60 of epoch 15\n",
      "Processing batch 7/60 of epoch 15\n",
      "Processing batch 8/60 of epoch 15\n",
      "Processing batch 9/60 of epoch 15\n",
      "Processing batch 10/60 of epoch 15\n",
      "Processing batch 11/60 of epoch 15\n",
      "Processing batch 12/60 of epoch 15\n",
      "Processing batch 13/60 of epoch 15\n",
      "Processing batch 14/60 of epoch 15\n",
      "Processing batch 15/60 of epoch 15\n",
      "Processing batch 16/60 of epoch 15\n",
      "Processing batch 17/60 of epoch 15\n",
      "Processing batch 18/60 of epoch 15\n",
      "Processing batch 19/60 of epoch 15\n",
      "Processing batch 20/60 of epoch 15\n",
      "Processing batch 21/60 of epoch 15\n",
      "Processing batch 22/60 of epoch 15\n",
      "Processing batch 23/60 of epoch 15\n",
      "Processing batch 24/60 of epoch 15\n",
      "Processing batch 25/60 of epoch 15\n",
      "Processing batch 26/60 of epoch 15\n",
      "Processing batch 27/60 of epoch 15\n",
      "Processing batch 28/60 of epoch 15\n",
      "Processing batch 29/60 of epoch 15\n",
      "Processing batch 30/60 of epoch 15\n",
      "Processing batch 31/60 of epoch 15\n",
      "Processing batch 32/60 of epoch 15\n",
      "Processing batch 33/60 of epoch 15\n",
      "Processing batch 34/60 of epoch 15\n",
      "Processing batch 35/60 of epoch 15\n",
      "Processing batch 36/60 of epoch 15\n",
      "Processing batch 37/60 of epoch 15\n",
      "Processing batch 38/60 of epoch 15\n",
      "Processing batch 39/60 of epoch 15\n",
      "Processing batch 40/60 of epoch 15\n",
      "Processing batch 41/60 of epoch 15\n",
      "Processing batch 42/60 of epoch 15\n",
      "Processing batch 43/60 of epoch 15\n",
      "Processing batch 44/60 of epoch 15\n",
      "Processing batch 45/60 of epoch 15\n",
      "Processing batch 46/60 of epoch 15\n",
      "Processing batch 47/60 of epoch 15\n",
      "Processing batch 48/60 of epoch 15\n",
      "Processing batch 49/60 of epoch 15\n",
      "Processing batch 50/60 of epoch 15\n",
      "Processing batch 51/60 of epoch 15\n",
      "Processing batch 52/60 of epoch 15\n",
      "Processing batch 53/60 of epoch 15\n",
      "Processing batch 54/60 of epoch 15\n",
      "Processing batch 55/60 of epoch 15\n",
      "Processing batch 56/60 of epoch 15\n",
      "Processing batch 57/60 of epoch 15\n",
      "Processing batch 58/60 of epoch 15\n",
      "Processing batch 59/60 of epoch 15\n",
      "Processing batch 60/60 of epoch 15\n",
      "Epoch [15/100], Loss: 100.0000\n",
      "Processing batch 1/60 of epoch 16\n",
      "Processing batch 2/60 of epoch 16\n",
      "Processing batch 3/60 of epoch 16\n",
      "Processing batch 4/60 of epoch 16\n",
      "Processing batch 5/60 of epoch 16\n",
      "Processing batch 6/60 of epoch 16\n",
      "Processing batch 7/60 of epoch 16\n",
      "Processing batch 8/60 of epoch 16\n",
      "Processing batch 9/60 of epoch 16\n",
      "Processing batch 10/60 of epoch 16\n",
      "Processing batch 11/60 of epoch 16\n",
      "Processing batch 12/60 of epoch 16\n",
      "Processing batch 13/60 of epoch 16\n",
      "Processing batch 14/60 of epoch 16\n",
      "Processing batch 15/60 of epoch 16\n",
      "Processing batch 16/60 of epoch 16\n",
      "Processing batch 17/60 of epoch 16\n",
      "Processing batch 18/60 of epoch 16\n",
      "Processing batch 19/60 of epoch 16\n",
      "Processing batch 20/60 of epoch 16\n",
      "Processing batch 21/60 of epoch 16\n",
      "Processing batch 22/60 of epoch 16\n",
      "Processing batch 23/60 of epoch 16\n",
      "Processing batch 24/60 of epoch 16\n",
      "Processing batch 25/60 of epoch 16\n",
      "Processing batch 26/60 of epoch 16\n",
      "Processing batch 27/60 of epoch 16\n",
      "Processing batch 28/60 of epoch 16\n",
      "Processing batch 29/60 of epoch 16\n",
      "Processing batch 30/60 of epoch 16\n",
      "Processing batch 31/60 of epoch 16\n",
      "Processing batch 32/60 of epoch 16\n",
      "Processing batch 33/60 of epoch 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# print(f'Output shape: {outputs.shape}, Labels shape: {labels.shape}')\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# print(f'Output: {outputs[:5]}, Labels: {labels[:5]}')\u001B[39;00m\n\u001B[32m     24\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m optimizer.step()\n\u001B[32m     27\u001B[39m train_losses.append(loss.item())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Coding stuff\\ADML\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Coding stuff\\ADML\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Coding stuff\\ADML\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T10:05:07.395016Z",
     "start_time": "2025-07-28T10:04:50.082726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the train set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_dataset = PETDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_train = []\n",
    "    y_true_train = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_train.extend(outputs.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "    y_pred_train = (torch.tensor(\n",
    "        y_pred_train) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_train = torch.tensor(y_true_train).numpy()\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "    print(\n",
    "        f'Train Accuracy: {train_accuracy:.4f}, Train F1 Score: {train_f1:.4f}, Train ROC AUC: {train_roc_auc:.4f}')"
   ],
   "id": "899939ec96e0aa0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.4202, Train F1 Score: 0.5917, Train ROC AUC: 0.5000\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T10:05:19.429902Z",
     "start_time": "2025-07-28T10:05:16.082373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    test_dataset = PETDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_test.extend(outputs.cpu().numpy())\n",
    "        y_true_test.extend(labels.cpu().numpy())\n",
    "    y_pred_test = (torch.tensor(\n",
    "        y_pred_test) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_test = torch.tensor(y_true_test).numpy()\n",
    "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    print(\n",
    "        f'Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}')"
   ],
   "id": "bfff8fa9307e6323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4333, Test F1 Score: 0.6047, Test ROC AUC: 0.5000\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('weights/best_deeppet_tuned.pth', weights_only=True))"
   ],
   "id": "c1ad185baea0226e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute the accuracy on the train set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_dataset = PETDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_train = []\n",
    "    y_true_train = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_train.extend(outputs.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "    y_pred_train = (torch.tensor(\n",
    "        y_pred_train) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_train = torch.tensor(y_true_train).numpy()\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "    print(\n",
    "        f'Train Accuracy: {train_accuracy:.4f}, Train F1 Score: {train_f1:.4f}, Train ROC AUC: {train_roc_auc:.4f}')"
   ],
   "id": "2aa0b4ecf1f0cc81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    test_dataset = PETDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        y_pred_test.extend(outputs.cpu().numpy())\n",
    "        y_true_test.extend(labels.cpu().numpy())\n",
    "    y_pred_test = (torch.tensor(\n",
    "        y_pred_test) > 0.5).float().numpy()  # Convert probabilities to binary predictions\n",
    "    y_true_test = torch.tensor(y_true_test).numpy()\n",
    "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_true_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_true_test, y_pred_test)\n",
    "    print(\n",
    "        f'Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}')"
   ],
   "id": "c8f7192c3f40fc70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
