{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:29:12.179505Z",
     "start_time": "2025-07-24T11:29:12.174538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import os\n",
    "from nilearn.image import load_img\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "data_dir = 'data'\n",
    "pet_dir = \"data/AD_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "747bf61390b1b9ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:26.235099Z",
     "start_time": "2025-07-24T11:27:23.205742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient: 002_S_5018\n",
      "Processing patient: 003_S_4136\n",
      "Processing patient: 003_S_4152\n",
      "Processing patient: 003_S_4373\n",
      "Processing patient: 003_S_4892\n",
      "Processing patient: 003_S_5165\n",
      "Processing patient: 003_S_5187\n",
      "Processing patient: 005_S_4707\n",
      "Processing patient: 005_S_4910\n",
      "Processing patient: 005_S_5038\n",
      "Processing patient: 005_S_5119\n",
      "Processing patient: 006_S_4153\n",
      "Processing patient: 006_S_4192\n",
      "Processing patient: 006_S_4546\n",
      "Processing patient: 006_S_4867\n",
      "Processing patient: 007_S_4568\n",
      "Processing patient: 007_S_4637\n",
      "Processing patient: 007_S_4911\n",
      "Processing patient: 007_S_5196\n",
      "Processing patient: 009_S_5027\n",
      "Processing patient: 009_S_5037\n",
      "Processing patient: 009_S_5224\n",
      "Processing patient: 009_S_5252\n",
      "Processing patient: 011_S_4827\n",
      "Processing patient: 011_S_4845\n",
      "Processing patient: 011_S_4906\n",
      "Processing patient: 011_S_4912\n",
      "Processing patient: 011_S_4949\n",
      "Processing patient: 013_S_5071\n",
      "Processing patient: 014_S_4039\n",
      "Processing patient: 014_S_4615\n",
      "Processing patient: 016_S_4009\n",
      "Processing patient: 016_S_4353\n",
      "Processing patient: 016_S_4583\n",
      "Processing patient: 016_S_4591\n",
      "Processing patient: 016_S_4887\n",
      "Processing patient: 016_S_4963\n",
      "Processing patient: 016_S_5032\n",
      "Processing patient: 016_S_5057\n",
      "Processing patient: 016_S_5251\n",
      "Processing patient: 018_S_4696\n",
      "Processing patient: 018_S_5240\n",
      "Processing patient: 019_S_4252\n",
      "Processing patient: 019_S_4477\n",
      "Processing patient: 019_S_4549\n",
      "Processing patient: 019_S_5012\n",
      "Processing patient: 019_S_5019\n",
      "Processing patient: 021_S_4718\n",
      "Processing patient: 021_S_4924\n",
      "Processing patient: 023_S_4501\n",
      "Processing patient: 023_S_5120\n",
      "Processing patient: 023_S_5241\n",
      "Processing patient: 024_S_4223\n",
      "Processing patient: 024_S_4280\n",
      "Processing patient: 024_S_4905\n",
      "Processing patient: 024_S_5054\n",
      "Processing patient: 027_S_4801\n",
      "Processing patient: 027_S_4802\n",
      "Processing patient: 027_S_4938\n",
      "Processing patient: 027_S_4962\n",
      "Processing patient: 027_S_4964\n",
      "Processing patient: 029_S_4307\n",
      "Processing patient: 031_S_4024\n",
      "Processing patient: 032_S_4755\n",
      "Processing patient: 033_S_5013\n",
      "Processing patient: 033_S_5017\n",
      "Processing patient: 033_S_5087\n",
      "Processing patient: 035_S_4783\n",
      "Processing patient: 036_S_4740\n",
      "Processing patient: 036_S_4820\n",
      "Processing patient: 036_S_4894\n",
      "Processing patient: 036_S_5063\n",
      "Processing patient: 036_S_5112\n",
      "Processing patient: 036_S_5210\n",
      "Processing patient: 037_S_4001\n",
      "Processing patient: 037_S_4770\n",
      "Processing patient: 037_S_4879\n",
      "Processing patient: 037_S_5162\n",
      "Processing patient: 051_S_4980\n",
      "Processing patient: 051_S_5005\n",
      "Processing patient: 052_S_4959\n",
      "Processing patient: 052_S_5062\n",
      "Processing patient: 053_S_5070\n",
      "Processing patient: 053_S_5208\n",
      "Processing patient: 067_S_4728\n",
      "Processing patient: 067_S_5205\n",
      "Processing patient: 068_S_4859\n",
      "Processing patient: 068_S_4968\n",
      "Processing patient: 068_S_5146\n",
      "Processing patient: 070_S_4692\n",
      "Processing patient: 070_S_4719\n",
      "Processing patient: 073_S_4853\n",
      "Processing patient: 073_S_5016\n",
      "Processing patient: 073_S_5090\n",
      "Processing patient: 082_S_5029\n",
      "Processing patient: 082_S_5184\n",
      "Processing patient: 094_S_4089\n",
      "Processing patient: 094_S_4282\n",
      "Processing patient: 094_S_4737\n",
      "Processing patient: 098_S_4095\n",
      "Processing patient: 098_S_4201\n",
      "Processing patient: 098_S_4215\n",
      "Processing patient: 099_S_4994\n",
      "Processing patient: 100_S_5106\n",
      "Processing patient: 114_S_4379\n",
      "Processing patient: 116_S_4195\n",
      "Processing patient: 116_S_4209\n",
      "Processing patient: 116_S_4338\n",
      "Processing patient: 116_S_4625\n",
      "Processing patient: 116_S_4732\n",
      "Processing patient: 123_S_4526\n",
      "Processing patient: 126_S_4494\n",
      "Processing patient: 126_S_4686\n",
      "Processing patient: 127_S_4500\n",
      "Processing patient: 127_S_4940\n",
      "Processing patient: 127_S_4992\n",
      "Processing patient: 127_S_5028\n",
      "Processing patient: 127_S_5056\n",
      "Processing patient: 127_S_5058\n",
      "Processing patient: 127_S_5067\n",
      "Processing patient: 127_S_5095\n",
      "Processing patient: 128_S_4772\n",
      "Processing patient: 128_S_4774\n",
      "Processing patient: 128_S_4792\n",
      "Processing patient: 128_S_5123\n",
      "Processing patient: 130_S_4589\n",
      "Processing patient: 130_S_4641\n",
      "Processing patient: 130_S_4660\n",
      "Processing patient: 130_S_4730\n",
      "Processing patient: 130_S_4971\n",
      "Processing patient: 130_S_4982\n",
      "Processing patient: 130_S_4984\n",
      "Processing patient: 130_S_4990\n",
      "Processing patient: 130_S_4997\n",
      "Processing patient: 130_S_5006\n",
      "Processing patient: 130_S_5059\n",
      "Processing patient: 130_S_5231\n",
      "Processing patient: 131_S_5138\n",
      "Processing patient: 135_S_4657\n",
      "Processing patient: 135_S_4676\n",
      "Processing patient: 135_S_4863\n",
      "Processing patient: 135_S_4954\n",
      "Processing patient: 135_S_5015\n",
      "Processing patient: 135_S_5275\n",
      "Processing patient: 137_S_4211\n",
      "Processing patient: 137_S_4258\n",
      "Processing patient: 137_S_4672\n",
      "Processing patient: 137_S_4756\n",
      "Processing patient: 153_S_4172\n"
     ]
    }
   ],
   "source": [
    "files = {}\n",
    "for file in os.listdir(pet_dir):\n",
    "    if file.endswith(\".nii\"):\n",
    "        img = load_img(os.path.join(pet_dir, file))\n",
    "        patient_id = file.split(\".\")[0].removeprefix('AD_normalised_')\n",
    "        print(f'Processing patient: {patient_id}')\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        torch_img = torch.tensor(img.get_fdata(), dtype=torch.float32)\n",
    "        files[patient_id] =torch_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79ae1aae983dd038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:26.837925Z",
     "start_time": "2025-07-24T11:27:26.648274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huw\\AppData\\Local\\Temp\\ipykernel_30924\\3852086957.py:1: DtypeWarning: Columns (19,20,21,50,51,104,105,106) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(data_dir, 'ADNIMERGE_19Jun2025.csv'))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'ADNIMERGE_19Jun2025.csv'))\n",
    "sex_df = df.filter(['PTID', 'PTGENDER'])\n",
    "sex_map = {'Male': 0, 'Female': 1}\n",
    "sex_labels = {0: 'Male', 1: 'Female'}\n",
    "sex_df['PTGENDER'] = sex_df['PTGENDER'].map(sex_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a374d69321325eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:29:18.845508Z",
     "start_time": "2025-07-24T11:29:18.819099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "696db8e9b1a77e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:30.126855Z",
     "start_time": "2025-07-24T11:27:30.103038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing patients: 15684\n",
      "Common patients: 737\n",
      "Total patients 149\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of common patients between the PET files and the sex_df\n",
    "missing_patients = sex_df[~sex_df['PTID'].isin(files.keys())]\n",
    "print(f'Missing patients: {len(missing_patients)}')\n",
    "\n",
    "common_patients = sex_df['PTID'].isin(files.keys())\n",
    "print(f'Common patients: {common_patients.sum()}')\n",
    "\n",
    "print(f'Total patients {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6972044d43a0ea82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:31.299399Z",
     "start_time": "2025-07-24T11:27:31.114519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update the sex DataFrame to include a new column for the PET image data matched on PTID\n",
    "for patient in files:\n",
    "    img = files.get(patient)\n",
    "    sex_df['PET_IMAGE'] = sex_df['PTID'].map(files) # Insert the img data ino the 'PET_IMAGE' column in sex_df for the corresponding PTID field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f18888aef471ede9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:27:31.995765Z",
     "start_time": "2025-07-24T11:27:31.987039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 16421\n",
      "Number of patients with PET images: 149\n"
     ]
    }
   ],
   "source": [
    "# Print the PTID for the columns for which PET_IMAGE is not None\n",
    "print(f'Number of patients: {len(sex_df)}')\n",
    "sex_df.dropna(subset=['PET_IMAGE'], inplace=True)\n",
    "sex_df.drop_duplicates(subset=['PTID'], inplace=True)\n",
    "print(f'Number of patients with PET images: {len(sex_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9176747064842ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:28:27.690636Z",
     "start_time": "2025-07-24T11:28:27.684330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First patient PTID: 135_S_5275\n",
      "PET image shape: torch.Size([101, 116, 96]) with data type <class 'torch.Tensor'>\n",
      "Sex: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the PET image for the first patient\n",
    "first_patient = sex_df.iloc[0]\n",
    "print(f'First patient PTID: {first_patient[\"PTID\"]}')\n",
    "print(f'PET image shape: {first_patient[\"PET_IMAGE\"].shape} with data type {type(first_patient[\"PET_IMAGE\"])}')\n",
    "print(f'Sex: {first_patient[\"PTGENDER\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f04aba63a8c550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:30:14.393862Z",
     "start_time": "2025-07-24T11:30:14.388023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X length: 149, PET image shape: torch.Size([101, 116, 96]), y shape: (149,)\n"
     ]
    }
   ],
   "source": [
    "# Select PET_IMAGE and PTGENDER from sex_df\n",
    "X = sex_df['PET_IMAGE'].tolist()  # This will be a list of torch.Tensor objects\n",
    "y = sex_df['PTGENDER'].values     # This will be a numpy array of labels\n",
    "\n",
    "print(f'X length: {len(X)}, PET image shape: {X[0].shape}, y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26f6414232ff88f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:30:23.594869Z",
     "start_time": "2025-07-24T11:30:23.587993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X length: 119, Test X length: 30 with shapes torch.Size([101, 116, 96]), torch.Size([101, 116, 96])\n",
      "Train y shape: (119,), Test y shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "print(f'Train X length: {len(X_train)}, Test X length: {len(X_test)} with shapes {X_train[0].shape}, {X_test[0].shape}')\n",
    "print(f'Train y shape: {y_train.shape}, Test y shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4c1e2a517af1973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:11.911733Z",
     "start_time": "2025-07-24T11:33:11.903722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a 3D CNN to deal with images of shape (101, 116, 96)\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(1, 16, kernel_size=5, padding='valid')\n",
    "        self.conv2 = torch.nn.Conv3d(16, 32, kernel_size=5, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(32 * 101 * 116 * 96 // 4 // 4 // 4, 128)\n",
    "        # self.fc2 = torch.nn.Linear(128, 2)  # Binary classification\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool3d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be1cbe60d3816f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "inputs = torch.stack([torch.unsqueeze(img, 0) for img in X_train], dim=0)  # Add channel dimension\n",
    "labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Convert labels to float and add a channel dimension\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Convert the training data to PyTorch tensors\n",
    "    # inputs = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(device)  # Add channel dimension\n",
    "    # labels = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254de356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
